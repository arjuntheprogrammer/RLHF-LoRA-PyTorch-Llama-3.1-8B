--- trl/core.py
+++ trl/core.py
@@ -22,7 +22,28 @@
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.nn.utils.rnn import pad_sequence
-from transformers import top_k_top_p_filtering
+try:
+    from transformers import top_k_top_p_filtering
+except Exception:
+    from transformers.generation.logits_process import TopKLogitsWarper, TopPLogitsWarper
+
+    def top_k_top_p_filtering(
+        logits: torch.Tensor,
+        top_k: int = 0,
+        top_p: float = 1.0,
+        filter_value: float = -float("Inf"),
+        min_tokens_to_keep: int = 1,
+    ) -> torch.Tensor:
+        # Compatibility shim for newer Transformers that dropped top_k_top_p_filtering.
+        if top_k > 0:
+            logits = TopKLogitsWarper(top_k, filter_value=filter_value, min_tokens_to_keep=min_tokens_to_keep)(
+                None, logits
+            )
+        if top_p < 1.0:
+            logits = TopPLogitsWarper(top_p, filter_value=filter_value, min_tokens_to_keep=min_tokens_to_keep)(
+                None, logits
+            )
+        return logits
 
 from .import_utils import is_npu_available, is_xpu_available
 
